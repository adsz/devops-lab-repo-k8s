---
name: "Kubernetes Disaster Recovery and Infrastructure Management"

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'backup'
        type: choice
        options:
          - backup
          - restore
          - snapshot_create
          - snapshot_restore
          - snapshot_list
          - cluster_health_check
          - disaster_recovery_test
      snapshot_name:
        description: 'Snapshot name (for snapshot operations)'
        required: false
        default: 'auto-$(date +%Y%m%d-%H%M%S)'
      restore_point:
        description: 'Restore point (snapshot name or backup date)'
        required: false
        default: 'latest'

  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
    - cron: '0 */6 * * *'  # Every 6 hours for health checks

env:
  ANSIBLE_HOST_KEY_CHECKING: false
  ANSIBLE_STDOUT_CALLBACK: yaml
  CONFIG_PATH: github-actions/config
  SCRIPTS_PATH: github-actions/scripts

jobs:
  setup:
    runs-on: self-hosted
    outputs:
      action: ${{ steps.determine_action.outputs.action }}
      snapshot_name: ${{ steps.determine_action.outputs.snapshot_name }}
      restore_point: ${{ steps.determine_action.outputs.restore_point }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Determine action to perform
        id: determine_action
        run: |
          if [ "${{ github.event_name }}" = "schedule" ]; then
            if [ "$(date +%H)" = "02" ]; then
              echo "action=backup" >> $GITHUB_OUTPUT
            else
              echo "action=cluster_health_check" >> $GITHUB_OUTPUT
            fi
            echo "snapshot_name=auto-$(date +%Y%m%d-%H%M%S)" >> \
              $GITHUB_OUTPUT
            echo "restore_point=latest" >> $GITHUB_OUTPUT
          else
            echo "action=${{ github.event.inputs.action }}" >> \
              $GITHUB_OUTPUT
            echo "snapshot_name=${{ github.event.inputs.snapshot_name }}" \
              >> $GITHUB_OUTPUT
            echo "restore_point=${{ github.event.inputs.restore_point }}" \
              >> $GITHUB_OUTPUT
          fi

      - name: Setup configuration
        run: |
          cd ubuntu-22.04
          chmod +x ${{ env.SCRIPTS_PATH }}/setup-config.sh
          ./${{ env.SCRIPTS_PATH }}/setup-config.sh

  vm_snapshot_operations:
    runs-on: self-hosted
    needs: setup
    if: >
      contains(fromJson('["snapshot_create", "snapshot_restore",
      "snapshot_list"]'), needs.setup.outputs.action)
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: VM Snapshot Operations
        run: |
          cd ubuntu-22.04
          chmod +x ${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh
          ./${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh \
            "${{ needs.setup.outputs.action }}" \
            "${{ needs.setup.outputs.snapshot_name }}"

  cluster_health_check:
    runs-on: self-hosted
    needs: setup
    if: >
      contains(fromJson('["cluster_health_check", "backup", "restore",
      "disaster_recovery_test"]'), needs.setup.outputs.action)
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python environment
        run: |
          cd ubuntu-22.04
          python3 -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt

      - name: Test connectivity to all nodes
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible all -i inventory.yml -m ping

      - name: Check Kubernetes cluster status
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get nodes -o wide" --become-user ansible

      - name: Check system pods status
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get pods -n kube-system" --become-user ansible

      - name: Check load balancer status
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible k8s-loadbalancers -i inventory.yml -m shell \
            -a "systemctl status haproxy keepalived" --become

      - name: Check disk space on all nodes
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible all -i inventory.yml -m shell -a "df -h /"

      - name: Check memory usage on all nodes
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible all -i inventory.yml -m shell -a "free -h"

      - name: Generate health report
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          echo "=== Cluster Health Report $(date) ===" > \
            /tmp/health_report.txt
          echo "Nodes Status:" >> /tmp/health_report.txt
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get nodes" --become-user ansible >> \
            /tmp/health_report.txt 2>&1 || echo "Failed to get nodes" >> \
            /tmp/health_report.txt
          echo "System Pods:" >> /tmp/health_report.txt
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get pods -n kube-system --no-headers | \
            grep -v Running | wc -l" --become-user ansible >> \
            /tmp/health_report.txt 2>&1 || echo "Failed to check pods" >> \
            /tmp/health_report.txt
          cat /tmp/health_report.txt

  backup:
    runs-on: self-hosted
    needs: [setup, cluster_health_check]
    if: >
      contains(fromJson('["backup", "disaster_recovery_test"]'),
      needs.setup.outputs.action)
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python environment
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate

      - name: Create VM snapshot before backup
        run: |
          cd ubuntu-22.04
          chmod +x ${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh
          ./${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh snapshot_create \
            "backup-${{ needs.setup.outputs.snapshot_name }}"

      - name: Backup Kubernetes cluster to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          cd ubuntu-22.04
          chmod +x ${{ env.SCRIPTS_PATH }}/backup_k8s_to_s3.sh
          ./${{ env.SCRIPTS_PATH }}/backup_k8s_to_s3.sh

      - name: Verify backup integrity
        run: |
          cd ubuntu-22.04
          echo "Verifying backup was created successfully..."
          aws s3 ls s3://${{ secrets.S3_BUCKET }}/k8s-backups/ \
            --recursive | tail -10

  restore:
    runs-on: self-hosted
    needs: setup
    if: needs.setup.outputs.action == 'restore'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python environment
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate

      - name: Confirm restore operation
        run: |
          echo "WARNING: This will restore the cluster to point: \
            ${{ needs.setup.outputs.restore_point }}"
          echo "Current cluster state will be lost!"
          echo "Proceeding with restore..."

      - name: Stop Kubernetes services
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          ansible k8s_masters,k8s_workers -i inventory.yml -m shell \
            -a "systemctl stop kubelet" --become || true

      - name: Restore from VM snapshot if specified
        run: |
          cd ubuntu-22.04
          if [[ "${{ needs.setup.outputs.restore_point }}" =~ ^snap-.* ]]
          then
            chmod +x ${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh
            ./${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh \
              snapshot_restore "${{ needs.setup.outputs.restore_point }}"
            sleep 300  # Wait for VMs to boot
          fi

      - name: Restore from S3 backup
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          cd ubuntu-22.04
          echo "Restoring from S3 backup..."
          # Implementation depends on backup script structure

      - name: Verify cluster after restore
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          sleep 180  # Wait for cluster to stabilize
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get nodes" --become-user ansible

  disaster_recovery_test:
    runs-on: self-hosted
    needs: [setup, backup]
    if: needs.setup.outputs.action == 'disaster_recovery_test'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run disaster recovery simulation
        run: |
          cd ubuntu-22.04
          echo "=== Disaster Recovery Test $(date) ==="
          echo "1. Creating test snapshot..."
          chmod +x ${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh
          ./${{ env.SCRIPTS_PATH }}/vm-snapshot-manager.sh snapshot_create \
            "dr-test-$(date +%Y%m%d-%H%M%S)"

      - name: Simulate failure and recovery
        run: |
          cd ubuntu-22.04
          source .venv/bin/activate
          echo "2. Simulating worker node failure..."
          ansible k8s-worker-1 -i inventory.yml -m shell \
            -a "systemctl stop kubelet" --become || true
          sleep 60
          echo "3. Checking cluster resilience..."
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get nodes" --become-user ansible
          echo "4. Recovering worker node..."
          ansible k8s-worker-1 -i inventory.yml -m shell \
            -a "systemctl start kubelet" --become || true
          sleep 60
          ansible k8s-master-1 -i inventory.yml -m shell \
            -a "kubectl get nodes" --become-user ansible

  notification:
    runs-on: self-hosted
    needs: [setup, cluster_health_check, backup, restore,
            disaster_recovery_test, vm_snapshot_operations]
    if: always()
    steps:
      - name: Send notification
        run: |
          ACTION="${{ needs.setup.outputs.action }}"
          STATUS="SUCCESS"
          if [[ "${{ needs.cluster_health_check.result }}" == "failure" ]] ||
             [[ "${{ needs.backup.result }}" == "failure" ]] ||
             [[ "${{ needs.restore.result }}" == "failure" ]] ||
             [[ "${{ needs.disaster_recovery_test.result }}" == "failure" ]] ||
             [[ "${{ needs.vm_snapshot_operations.result }}" == "failure" ]]
          then
            STATUS="FAILURE"
          fi
          echo "Action: $ACTION completed with status: $STATUS"